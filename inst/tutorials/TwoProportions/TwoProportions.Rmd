---
title: "Tests and CIs for Two Proportions"
output: 
  learnr::tutorial:
    progressive: TRUE
    allow_skip: TRUE
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
# library(checkr)
# library(statPREP)
library(tibble)
library(ggformula)
library(mosaic)
theme_set(theme_bw(base_size=16))
# knitr::opts_chunk$set(exercise.checker = checkr::checkr_tutor)
knitr::opts_chunk$set(
  echo = FALSE,
  fig.align = "center",
  fig.width = 6, fig.height = 2.5)
tutorial_options(exercise.eval = FALSE)
island_eyes <- read.csv('https://goo.gl/39X1G7', strip.white = TRUE) %>%
  na.omit()
```

## Learning Outcomes

This module focuses on a **shortcut for constructing CIs and doing hypothesis tests for two proportions.** "Two proportions" refers to problems that involve comparing proportions between two groups.  You already know how to do these tasks using bootstrapping and randomization, but now you'll learn a shortcut that allows you to skip the resampling. *We'll again call this shortcut the* **Normal Model**. You will use key content from previous sections, including:

- Understanding and interpretation of sampling distributions
- Using normal distributions to compute $z^*$ values or p-values, given the standard error of the relevant sampling distribution

At the end of this module, you will:

1. Be practiced at using the **Normal Model** for two proportions. You'll learn another "shortcut" formula for computing the SE for a test or CI from sample data, without resampling, when the parameter of interest is **two proportions**
2. Be familiar with the process of constructing CIs, or finding and interpreting p-values, using this shortcut

## Text Book Reference

This module covers **Lock5 Chapter 6.7 - 6.9**. Relevant sections will be noted throughout this tutorial when they are covered.

*It is recommended that you read the text book sections either before or after completing the corresponding parts of the tutorial, especially for any sections you feel less confident about.*

## Dataset
To extend our knowledge from the *normal model for one proportion* to the *normal model for two proportions*, we will use an example dataset from [The Islands](https://islands.smp.uq.edu.au/). It was collected by previous intro stat students - I'm sure you're grateful for their work! The dataset is available at:

<https://goo.gl/39X1G7>

The dataset, called `island_eyes`, contains 200 cases and variables including:

- `EyeColor` (values "Brown" and "Not Brown")
- `Island` (values "South" and "North")

We can use it to answer: Is the proportion Islanders with brown eyes *the same* on the North and South Islands?

This is a *two-proportion* problem because our parameter of interest is a **difference in proportions between two groups** -- the proportion is the proportion people with brown eyes, and the two groups are North Island residents and South Island residents.

## SE for a Two Proportions
*Text book reference: [IMS Ch. 6.2](https://openintro-ims.netlify.app/inference-cat.html#diff-two-prop)*

Just as the Central Limit Theorem for one proportion told us that the sampling distribution for a proportion is normal, with SE = $\sqrt{\frac{p(1-p)}{n}}$, there's a CLT for *two* proportions that gives us equally valuable information.

### Central Limit Theorem
The Central Limit Theorem (CLT), tells us that the sampling distribution for two proportions **will be normal** *as long as the sample size is big enough.*

#### But...*how* **big** is *big enough, for the sample size?*
By logic similar to what we used for one proportion, we obtain a rule of thumb for how big a sample size is "big enough" for the normal model to apply, for two proportions. (Basically, it's the same rule...just applied to each group).

*Our handy rule of thumb says:*

**If $np \geq 10$ and $n(1-p) \geq 10$** *for each group,* **the sampling distribution will be normal.**

*Another way of saying the same thing is below. Remember, if you're computing proportions, you must have data in a categorical variable with two possible categories (and a second categorical variable specifying two groups).*

**If sample size is big enough that we expect at least 10 observations in each of our two categories, IN EACH OF OUR TWO GROUPS, then the sampling distribution will be normal.**

### Caution!
If you are working with data on one proportion to find a CI or do a hypothesis test, your sample size *must* meet the rule of thumb above ($np \geq 10$ and $n(1-p) \geq 10$) or you can't use the short cut we're about to learn!

## SE for two proportions: Formula
*Text book reference: [IMS Ch. 6.2](https://openintro-ims.netlify.app/inference-cat.html#diff-two-prop)*

Our exploration of the normal model for one proportion gives us a good idea of what we'd expect the formula for the SE for two proportions to look like. It'll look similar to the one for one proportion, but with a term for each of the two groups.

Without further ado, according to the Central Limit Theorem for two proportions...

If $n$ is big enough, then the sampling distribution is normal with standard deviation

$$ SE = \sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$$

Where $p_1$ is the true population proportion for one group, $p_2$ is the true population proportion for the other group, $n_1$ is the sample size in the first group, and $n_2$ is the sample size in the second group.

For a confidence interval, as you might guess, $\hat{p}_1$ and $\hat{p}_2$ (the sample proportions in the two groups) are our best estimates of $p_1$ and $p_2$, so we use *them* in the formula above to make calculations.

### Whoa!!

Now we have *even more* power -- we have a nifty shortcut for estimating the SE for problems about *two proportions* as well as problems about *one proportion*!

## Using your Power Wisely: CIs

As long as you know the sample sizes $n_1$ and $n_2$ and the sample proportions $\hat{p}_1$ and $\hat{p}_2$, you can now **estimate the SE for a CI without bootstrapping**.

$$ SE = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}$$

to get the SE, then

$$(\hat{p}_1 - \hat{p}_2) \pm z^*SE$$

to get the CI. (Remember -- from before -- $z^*$ is the multiplier for your chosen confidence level C, obtained from the boundaries of the central C% of the standard normal distribution.) 

For reference,

```{r, echo = FALSE}
ztab <- tibble(`Confidence Level` = c(80, 90, 95, 97, 98, 99))
ztab <- ztab %>% 
  mutate(`z*` = qnorm(1 - (1 - ztab$`Confidence Level`/100)/2))
pander::pander(ztab)
```

### Practice: Brown Eyed...Islanders, statistic

Let's practice: First, use R to compute the difference in proportion brown-eyed Islanders, between North and South Islands. Confirm that the sample stat $\hat{p}_S - \hat{p}_N = 0.0359$, where $\hat{p}_S$ is the proportion South Islanders with brown eyes, and $\hat{p}_N$ is the proportion North Islanders with brown eyes.


```{r eye-stat, exercise = TRUE}
island_eyes <- read.csv('https://goo.gl/39X1G7', strip.white = TRUE) %>%
  na.omit()

```

```{r eye-stat-hint-1}
island_eyes <- read.csv('https://goo.gl/39X1G7', strip.white = TRUE) %>%
  na.omit()
tally(..., data = island_eyes)
```

```{r eye-stat-hint-2}
island_eyes <- read.csv('https://goo.gl/39X1G7', strip.white = TRUE) %>%
  na.omit()
tally(~EyeColor | Island, data = island_eyes, format = 'prop')
```

```{r eye-stat-hint-3}
island_eyes <- read.csv('https://goo.gl/39X1G7', strip.white = TRUE) %>%
  na.omit()
tally(~EyeColor | Island, data = island_eyes, format = 'prop')[1,]
```

```{r eye-stat-hint-4}
island_eyes <- read.csv('https://goo.gl/39X1G7', strip.white = TRUE) %>%
  na.omit()
diff(tally(~EyeColor | Island, data = island_eyes, format = 'prop')[1,])
```

### Practice: Brown Eyed...Islanders, SE

Next, compute the standard error of the sampling distribution for this scenario. Remember, you'll use:

$$ SE = \sqrt{\frac{\hat{p}_1(1-\hat{p}_1)}{n_1} + \frac{\hat{p}_2(1-\hat{p}_2)}{n_2}}$$

```{r, brown-SE, exercise = TRUE}

```

```{r, brown-SE-solution}
phats <- tally(~EyeColor | Island, data = island_eyes, format = 'prop')[1,]
phat_N <- phats[1]
phat_S <- phats[2]
ns <- tally(~Island, data = island_eyes); ns
n_N <- ns[1]
n_S <- ns[2]
SE <- sqrt( phat_N * (1 - phat_N) / n_N + phat_S * (1 - phat_S) / n_S )
SE
```

### Practice: Brown-eyed Islanders, CI
Finally, choose a confidence level, and use it (with the sample stat  and SE from above) to find a CI for the difference in proportion brown eyes between North and South Islands. Report your results in context.


```{r brown-ci, exercise = TRUE}

```

```{r brown-ci-solution}
# note: this is for 95% confidence; change z* for other confidence level.
stat <- diff(tally(~EyeColor | Island, data = island_eyes, format = 'prop')[1,])
CI <- stat + c(-1,1) * 1.96 * 0.05687706
CI
```

**Solution:** We are 95% confident that the true difference in proportion brown-eyed islanders (South minus North Island) is between -0.041 and 0.181.  This means it is plausible that the proportion brown-eyed people is the same on both islands, since 0 (no difference) is in the CI.


## Using your Power Wisely: Tests

What if you wanted to answer, *"Is the proportion brown-eyed Islanders the same on North and South Islands"*, using a hypothesis test instead of a CI?

Before we get started, let's specify the hypotheses we'd want to test.


```{r stalwart-test}
quiz(question("What null hypothesis will we test?",
           answer("$H_0: p_N - p_S = 0$", correct = TRUE),
           answer("$H_0: \\hat{p}_n = \\hat{p}_S$", message = "Remember - hypotheses are always statements about *parameters*, not *statistics*."),
           answer("$H_0: p = 0$", message = "Not quite. Remember - There are two groups, so there should be two proportions appearing in your hypotheses."),
           answer("$H_0: \\hat{p} = 0.07$", message = "Oops. we *know* that the sample difference in proportions is 0.07 -- that is just a fact, not a testable hypothesis."),
           allow_retry = TRUE,
           random_answer_order = TRUE)
)
```

*Note: $H_A$ will be the same as $H_0$, but with a $\neq$ instead of $=$.*

Now...Think back to when we first learned randomization tests. To create a randomization sampling distribution, do you remember what we had to *assume* before we began? (*Yes, you answered this before -- but it's important again!*)

```{r, assume-h0}
quiz(question("What is the key, driving assumption when we start work to generate a sampling distribution for a hypothesis test?",
              answer("We assume the null hypothesis is true.", correct = TRUE),
              answer("We assume our sample is representative", message = "Yes, of course we do. But that isn't the main point here -- try again!"),
              answer("We assume that our sample size is big enough",
                     message = "We don't need to assume anything about sample size; if there are requirements related to sample size, we can jsut check them! Try again..."),
              random_answer_order = TRUE,
              allow_retry = TRUE)
)
```

### Implications

Wait a second.  If we are **assuming $H_0$ is true**, then suddenly (and wierdly!), $\hat{p}_S - \hat{p}_N$ is *no longer* our best estimate of the true difference in proportions $p_S - p_N$.

**$H_0$ tells us that $p_N$ and $p_S$ should be THE SAME.** 

How can we *use the data* to get estimates of $p_N$ and $p_S$ *under the assumption that $p_N = p_S$?*  It's a bit tricky - think it over for a moment!

### $p_{pooled}$

The estimator we want to use is $\hat{p}_{pooled}$ or, for short, $\hat{p}_p$. It is the sample proportion computed *as if the null hypothesis were true, and there is no difference between groups*. This means: we should *pool the two groups* (since they are supposedly the same) and find the **total number of brown eyed islanders** divided by the **total sample size**. In other words, we compute the proportion of interest *ignoring the groups entirely*.

Find $p_p$ for the `island_eyes` brown-eyed Islanders problem.

```{r p-pool, exercise = TRUE}

```

```{r p-pool-solution}
tally(~EyeColor | Island, data = island_eyes, margins = TRUE)
(21 + 24) / (91 + 90)
# or just:
prop(~EyeColor == 'Brown', data = island_eyes)
```

Now, **using your estimate $\hat{p}_p$,** compute the SE for this hypothesis test according to:

$$ SE = \sqrt{\frac{\hat{p}_{p}(1-\hat{p}_{p})}{n_1} + \frac{\hat{p}_{p}(1-\hat{p}_{p})}{n_2}}$$

```{r test-se, exercise = TRUE}

```

```{r test-se-solution}
pp <- prop(~EyeColor == 'Brown', data = island_eyes)
SE <- sqrt(pp * (1-pp) / 90 + pp * (1-pp) / 91)
SE
```

The standard error is 0.0643.

### Pause and Sketch
Before making any more calculations, pause and grab a piece of paper.

Sketch the sampling distribution under the null hypothesis (which says $p_N = p_S$). Include on your sketch the sample statistic ($\hat{p}_S - \hat{p}_N$), and the area corresponding to the p-value of your test.

Keep your sketch handy - you can check it against the `xpnorm()` output in the next section to make sure you got it right. (*Making sketches like these is a classic STAT 243 test question...*)

### P-value
Finally, compute the p-value of this test and formulate your conclusion.

```{r eye-test-work, exercise = TRUE}

```


```{r eye-test-work-solution}
stat <- 0.07
SE <- 0.05688
2 * xpnorm(stat, mean = 0, sd = SE, lower.tail = FALSE)
```

**Conclusion:** The p-value is quite large - about 0.22. So we fail to reject $H_0$. Our data provide no evidence that the proportion brown-eyed Islanders is different on the North and South Islands.


### Review: Brown-Eyed Randomization

As a review exercise, repeat the test you just did using a randomization test instead of the normal model ("shortcut" method).

```{r, stalwart-strategy}
quiz(question("What randomization strategy will work best in this case?",
              answer("rflip()", message = 'Not quite! rflip() ONLY works for ONE proportion (at least the simple version...).'),
              answer("shuffle()", correct = TRUE),
              answer("randomly flip signs of differences",
                     message = "Careful! These are not paired data, so the sign-flipping strategy won't work. Another way to say it: the data DO NOT contain a list of differences!"),
              answer("recenter and resample"),
              allow_retry = TRUE,
              random_answer_order = TRUE
  
))
```

```{r eye-rand, exercise = TRUE}

```

```{r eye-rand-solution}
rd <- do(1000)*diff(tally(~EyeColor | shuffle(Island),
                     data = island_eyes,
                     format = 'prop')[1,])
2 * prop(~South >= 0.07, data = rd)
gf_histogram(~South, data = rd, bins = 11,
             xlab = 'Difference in Proportions')
```

You should find that the p-values computed via the normal model and via the randomization test are pretty similar, and lead to the same conclusion (failure to reject $H_0$, and no evidence of a difference in proportion brown eyes by island).

## Conditions: When Can You Use The Power?

We already noted that, for the normal model for two proportions to work, the sample size must be big enough.

There are other conditions as well.  *They are the same as for one proportion, but let's review!*

First (as always) we can't make valid inferences about a population unless **we have a representative sample from the population of interest.**

In addition, this method requires that the **observations (individual cases in the dataset) should be independent**. This means there should be no important lurking variables; in other words, there's nothing that would make some observations more similar than others. One more way of putting it: there's no feature that helps you to predict one observation based on another.

To sum up, **You can use the normal model for two-proportion data if:**

1. The sample is representative of the population of interest. 
2. The sample size is big enough ($np \geq 10$ and $n(1-p) \geq 10$ *in each group*).
3. The observations are independent (aside from differences between the two groups).

## More Practice

You will practice the skills you just learned in Quiz 9 and Homework 12.

For *additional* mastery of one- and two-proportion problems, my recommendation is this:

- Write a brief outline detailing what you learned doing the one- and two-proportion tutorials.
- Make a flow chart showing how to estimate the SE for *CIs* and *tests* for *one* and *two* proportions. 
- Detail the steps you would follow if you had to solve a new problem using the normal model with proportion data. For example:
  - How do you tell if it's a one- or two-proportion problem?
  - If doing a test, how do you figure out what the hypotheses are?
  - How do you compute the SE for each difference case (see above)?
  - What would a sketch of the sampling distribution look like (where would it be centered, and what would its SE be)? Can you add the boundaries of the CI, or the area corresponding to the p-value, to the sketch?

## Conclusion

Before you go, give yourself a pat on the back. You just covered a BIG chunk of material! It may not be the most conceptually tough part of the class, but there is *a lot* of detail to keep track of.

<iframe src="https://giphy.com/embed/3o6gEbd93QQIt61us8" width="480" height="346" frameBorder="0" class="giphy-embed" allowFullScreen></iframe><p><a href="https://giphy.com/gifs/hallmarkecards-congratulations-lobster-you-did-it-3o6gEbd93QQIt61us8">via GIPHY</a></p>